{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amit507017/FedCollabNN/blob/master/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w92G-bgbQwXD"
      },
      "source": [
        "## Secure Aggregation of Feature Vectors [SAFE]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs_gX6Bp2vxt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ummyePYo2yXv"
      },
      "outputs": [],
      "source": [
        "n_users = 50000\n",
        "vector_size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJPDWNan21CM"
      },
      "outputs": [],
      "source": [
        "def sample_user_vectors(n_users, vector_size):\n",
        "  user_vectors = []\n",
        "  for i in range(n_users):\n",
        "    # user_vectors.append(np.random.rand(1, vector_size)[0])\n",
        "    if i>n_users/2:\n",
        "      user_vectors.append(np.random.rand(1, vector_size)[0])\n",
        "    else:\n",
        "      user_vectors.append(np.random.exponential(1,vector_size))\n",
        "  return user_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcixVonw-osU"
      },
      "outputs": [],
      "source": [
        "def sum_user_vectors(user_vectors):\n",
        "  sum_vector = np.zeros_like(user_vectors[0])\n",
        "  for i in range(n_users):\n",
        "    sum_vector = np.add(sum_vector, user_vectors[i])\n",
        "  return sum_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRhCprP8SsaD"
      },
      "outputs": [],
      "source": [
        "def avg_user_vectors(user_vectors):\n",
        "  sum_vector = np.zeros_like(user_vectors[0])\n",
        "  for i in range(len(user_vectors)):\n",
        "    sum_vector = np.add(sum_vector, user_vectors[i])\n",
        "  return sum_vector / len(user_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07HutQT_A-8M"
      },
      "outputs": [],
      "source": [
        "def create_shares(x, n_shares):\n",
        "  shares = list()\n",
        "  temp_sum = np.zeros_like(x)\n",
        "  for i in range(n_shares-1):\n",
        "    share = np.random.rand(len(x))\n",
        "    shares.append(share)\n",
        "    temp_sum = np.add(temp_sum, share)\n",
        "  shares.append(np.subtract(x, temp_sum))\n",
        "  return shares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePBbdUrPB5fS"
      },
      "outputs": [],
      "source": [
        "def create_total_shares(user_vectors):\n",
        "  total_shares = []\n",
        "  for i in range(len(user_vectors)):\n",
        "    shares = create_shares(user_vectors[i], len(user_vectors))\n",
        "    for share in shares:\n",
        "      total_shares.append(share)\n",
        "  return total_shares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9Ck4sULQIQt"
      },
      "outputs": [],
      "source": [
        "def sum_shares(total_shares):\n",
        "  sum_shares = np.zeros_like(total_shares[0])\n",
        "  for share in total_shares:\n",
        "    sum_shares = np.add(sum_shares, share)\n",
        "  return sum_shares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqc1KHpuIIGh",
        "outputId": "3cdc54e0-a7f2-4eb9-e4ec-de8a49a84cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[37352.08093431]\n"
          ]
        }
      ],
      "source": [
        "user_vectors = sample_user_vectors(n_users, vector_size)\n",
        "# total_shares = create_total_shares(user_vectors)\n",
        "# print(sum_shares(total_shares))\n",
        "print(sum_user_vectors(user_vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x28HeSNfSOkI",
        "outputId": "e0d6e49c-e1cc-4fea-f461-22cdb59691cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.74704162])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "avg_user_vectors(user_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "safe_means = []\n",
        "sigmas = []\n",
        "n_shares = 30\n",
        "\n",
        "for k in range(30):\n",
        "  means = []\n",
        "  for i in range(30):\n",
        "    round_users = random.sample(user_vectors, n_shares)\n",
        "    # if i==20:\n",
        "    #   for j in range(n_adv):\n",
        "    #     round_users[j] = np.array([50])\n",
        "    total_shares = create_total_shares(round_users)\n",
        "    means.append(np.mean(total_shares) * n_shares)\n",
        "  # print(max(means))\n",
        "  safe_means.append(np.mean(means))\n",
        "  sigmas.append(np.square(np.std(means)/np.sqrt(n_shares)))\n",
        "\n",
        "print(\"True Mean:\", np.mean(user_vectors))\n",
        "print(\"Estimated Mean\",np.mean(safe_means))\n",
        "\n",
        "sum_sigmas = 0\n",
        "for sigma in sigmas:\n",
        "  sum_sigmas = sum_sigmas + sigma\n",
        "\n",
        "print(\"Lindeberg condition\",max(sigmas)/sum_sigmas)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlX2_S7oBxtc",
        "outputId": "ebacbf2b-221e-428e-b748-4bbf1c551115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 0.7458831431538404\n",
            "Lindeberg condition 0.06806595602731691\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfhMY1SQQ2v5",
        "outputId": "20f3df95-22c7-4d39-e6ed-3360efa56a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 0.7496162988201818\n",
            "Lindeberg condition 0.05110807239240733\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 0.7704194971174357\n",
            "Lindeberg condition 0.04009307630492865\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 0.7989955385625653\n",
            "Lindeberg condition 0.0285401092353938\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 0.8271317547052187\n",
            "Lindeberg condition 0.023464532850345644\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 0.8555024740700647\n",
            "Lindeberg condition 0.01883175657860403\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 0.8828382215720202\n",
            "Lindeberg condition 0.016262567344179763\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 0.9110128667258371\n",
            "Lindeberg condition 0.014427738316969635\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 0.9387032087092477\n",
            "Lindeberg condition 0.012099707220419587\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 0.9662937088525807\n",
            "Lindeberg condition 0.010708516327912677\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 0.9942505040190834\n",
            "Lindeberg condition 0.00975153335764814\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.0217854462547358\n",
            "Lindeberg condition 0.008851073030401906\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.0491099600036502\n",
            "Lindeberg condition 0.008198106016523148\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.076643743338031\n",
            "Lindeberg condition 0.007625906778891219\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.1037050052422919\n",
            "Lindeberg condition 0.007059270865562847\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.1306725797388224\n",
            "Lindeberg condition 0.006610909855405508\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.1584155059819947\n",
            "Lindeberg condition 0.006157975974257517\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.1856244663665036\n",
            "Lindeberg condition 0.005787195291620126\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.213006335187977\n",
            "Lindeberg condition 0.005509611100537402\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.2400513118558085\n",
            "Lindeberg condition 0.005230896198710857\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.2672904939578213\n",
            "Lindeberg condition 0.004923961172290316\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.2947704278081822\n",
            "Lindeberg condition 0.004696093997654333\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.3224696508060279\n",
            "Lindeberg condition 0.004482045823157024\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.350186584716098\n",
            "Lindeberg condition 0.004276167863759536\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.3776798369752352\n",
            "Lindeberg condition 0.004099314474653002\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.4049560222578745\n",
            "Lindeberg condition 0.003945678561457163\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.4323632194377698\n",
            "Lindeberg condition 0.003792833330160907\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.4596954571038183\n",
            "Lindeberg condition 0.003655065914505197\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.4870994824011599\n",
            "Lindeberg condition 0.003520369764617045\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.5144016616731018\n",
            "Lindeberg condition 0.0034054467568775545\n",
            "\n",
            "\n",
            "True Mean: 0.7470416186862466\n",
            "Estimated Mean 1.541753845983047\n",
            "Lindeberg condition 0.0032811239464046126\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "safe_means = []\n",
        "sigmas = []\n",
        "n_shares = 30\n",
        "\n",
        "for n_adv in range(30):\n",
        "  for k in range(30):\n",
        "    means = []\n",
        "    for i in range(30):\n",
        "      round_users = random.sample(user_vectors, n_shares)\n",
        "      if i==20:\n",
        "        for j in range(n_adv):\n",
        "          round_users[j] = np.array([50])\n",
        "      total_shares = create_total_shares(round_users)\n",
        "      means.append(np.mean(total_shares) * n_shares)\n",
        "    # print(max(means))\n",
        "    safe_means.append(np.mean(means))\n",
        "    sigmas.append(np.square(np.std(means)/np.sqrt(n_shares)))\n",
        "\n",
        "  print(\"True Mean:\", np.mean(user_vectors))\n",
        "  print(\"Estimated Mean\",np.mean(safe_means))\n",
        "\n",
        "  sum_sigmas = 0\n",
        "  for sigma in sigmas:\n",
        "    sum_sigmas = sum_sigmas + sigma\n",
        "\n",
        "  print(\"Lindeberg condition\",max(sigmas)/sum_sigmas)\n",
        "  print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN Training with adversary"
      ],
      "metadata": {
        "id": "rGR96c_vCJzA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmoHwwfZPxoa"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6M81AOUPxob"
      },
      "outputs": [],
      "source": [
        "args={}\n",
        "kwargs={}\n",
        "args['batch_size']=1000\n",
        "args['test_batch_size']=1000\n",
        "args['epochs']=10  #The number of Epochs is the number of times you go through the full dataset. \n",
        "args['lr']=0.01 #Learning rate is how fast it will decend. \n",
        "args['momentum']=0.5 #SGD momentum (default: 0.5) Momentum is a moving average of our gradients (helps to keep direction).\n",
        "\n",
        "args['seed']=1 #random seed\n",
        "args['log_interval']=10\n",
        "args['cuda']=False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLys0rPDPxoc"
      },
      "outputs": [],
      "source": [
        "#load the data\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args['test_batch_size'], shuffle=True, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Zbs4BfSPxod"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    #This defines the structure of the NN.\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()  #Dropout\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Convolutional Layer/Pooling Layer/Activation\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) \n",
        "        #Convolutional Layer/Dropout/Pooling Layer/Activation\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        #Fully Connected Layer/Activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        #Fully Connected Layer/Activation\n",
        "        x = self.fc2(x)\n",
        "        #Softmax gets probabilities. \n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMsatZgpPxoe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if args['cuda']:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        #Variables in Pytorch are differenciable. \n",
        "        data, target = Variable(data), Variable(target)\n",
        "        #This will zero out the gradients for this batch. \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\n",
        "        loss = F.nll_loss(output, target)\n",
        "        #dloss/dx for every Variable \n",
        "        loss.backward()\n",
        "        # manipulate gradient\n",
        "        if epoch == 2 and batch_idx == 1:\n",
        "          for p in model.parameters():\n",
        "            p.grad += 0.1  # or whatever other operation\n",
        "            break\n",
        "        #to do a one-step update on our parameter.\n",
        "        optimizer.step()\n",
        "        #Print out the loss periodically. \n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        if args['cuda']:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data.item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(args['seed'])\n",
        "import random\n",
        "random.seed(0)\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "model = Net()\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "zNKY6679SBV5",
        "outputId": "e56d7239-fa69-4d10-9d75-afe889c58ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.356195\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.311571\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.287925\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.282732\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.250925\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.234330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1809, Accuracy: 3702/10000 (37%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.210009\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.168724\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.114218\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.064461\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.953717\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.895009\n",
            "\n",
            "Test set: Average loss: 1.5701, Accuracy: 6236/10000 (62%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.768301\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.675914\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.556124\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.521331\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.378313\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.278081\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-aacd1968131f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-fc4a8f480eb6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#This will zero out the gradients for this batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-9db81c519b9b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#Convolutional Layer/Pooling Layer/Activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m#Convolutional Layer/Dropout/Pooling Layer/Activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGqh9gaTPxoe",
        "outputId": "4ca78b0d-3223-43b1-b807-8951b3949910",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.322669\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.294595\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.284315\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.258211\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.242819\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.213622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.156195\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.116604\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.093298\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.029436\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.896298\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.804620\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.751431\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.668146\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.608938\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.450326\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.378610\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.370627\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.315881\n",
            "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.171311\n",
            "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.222647\n",
            "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.083317\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.052519\n",
            "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 1.018625\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.984425\n",
            "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.938229\n",
            "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.904740\n",
            "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.954368\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.916209\n",
            "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.899438\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.797516\n",
            "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.793168\n",
            "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.783918\n",
            "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.817132\n",
            "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.714123\n",
            "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.816043\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.703802\n",
            "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.755672\n",
            "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.716026\n",
            "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.742668\n",
            "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.712079\n",
            "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.715604\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.684570\n",
            "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.765269\n",
            "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.667188\n",
            "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.654596\n",
            "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.647389\n",
            "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.618610\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.660242\n",
            "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.604119\n",
            "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.553576\n",
            "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.621667\n",
            "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.597977\n",
            "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.600522\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.541191\n",
            "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.591110\n",
            "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.546683\n",
            "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.554835\n",
            "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.514429\n",
            "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.564813\n"
          ]
        }
      ],
      "source": [
        "model = Net()\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ioe0pGbF2rIz"
      },
      "source": [
        "## Syft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BC_aExD3TVI",
        "outputId": "1521947e-354b-440f-c55a-9939ffa9589b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting syft==0.2.9\n",
            "  Downloading syft-0.2.9-py3-none-any.whl (433 kB)\n",
            "\u001b[K     |████████████████████████████████| 433 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.0.4)\n",
            "Collecting websockets~=8.1.0\n",
            "  Downloading websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting requests-toolbelt==0.9.1\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 931 kB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask~=1.1.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (1.1.4)\n",
            "Collecting websocket-client~=0.57.0\n",
            "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 65.3 MB/s \n",
            "\u001b[?25hCollecting importlib-resources~=1.5.0\n",
            "  Downloading importlib_resources-1.5.0-py2.py3-none-any.whl (21 kB)\n",
            "Collecting torchvision~=0.5.0\n",
            "  Downloading torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 48.1 MB/s \n",
            "\u001b[?25hCollecting numpy~=1.18.1\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting lz4~=3.0.2\n",
            "  Downloading lz4-3.0.2-cp37-cp37m-manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 45.2 MB/s \n",
            "\u001b[?25hCollecting shaloop==0.2.1-alpha.11\n",
            "  Downloading shaloop-0.2.1_alpha.11-py3-none-manylinux1_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 53.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (7.1.2)\n",
            "Collecting phe~=1.4.0\n",
            "  Downloading phe-1.4.0.tar.gz (35 kB)\n",
            "Collecting tornado==4.5.3\n",
            "  Downloading tornado-4.5.3.tar.gz (484 kB)\n",
            "\u001b[K     |████████████████████████████████| 484 kB 47.1 MB/s \n",
            "\u001b[?25hCollecting notebook==5.7.8\n",
            "  Downloading notebook-5.7.8-py2.py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 38.4 MB/s \n",
            "\u001b[?25hCollecting aiortc==0.9.28\n",
            "  Downloading aiortc-0.9.28-cp37-cp37m-manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 68.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill~=0.3.1 in /usr/local/lib/python3.7/dist-packages (from syft==0.2.9) (0.3.5.1)\n",
            "Collecting syft-proto~=0.5.2\n",
            "  Downloading syft_proto-0.5.3-py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting tblib~=1.6.0\n",
            "  Downloading tblib-1.6.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting openmined.threepio==0.2.0\n",
            "  Downloading openmined.threepio-0.2.0.tar.gz (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting RestrictedPython~=5.0\n",
            "  Downloading RestrictedPython-5.2-py2.py3-none-any.whl (28 kB)\n",
            "Collecting flask-socketio~=4.2.1\n",
            "  Downloading Flask_SocketIO-4.2.1-py2.py3-none-any.whl (16 kB)\n",
            "Collecting torch~=1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 1.2 MB/s eta 0:00:01\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install syft==0.2.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "LNRAdeCt2qZ5",
        "outputId": "8a4d61ee-4102-47b1-a712-49a33344428f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-aa2bb954649b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msyft\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTorchHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'syft'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#Importing all the required libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "import syft as sy\n",
        "import time\n",
        "hook = sy.TorchHook(torch)\n",
        "import torch.nn.functional as F\n",
        "import statistics\n",
        "from syft.frameworks.torch.dp import pate\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K96bRyzc3O3V"
      },
      "outputs": [],
      "source": [
        "# Defining arguments necessary for different parts of FedCollabNN implmentation \n",
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 5\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "        self.epsilon = 1\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VncNCQ0o3ti9"
      },
      "outputs": [],
      "source": [
        "# Neural network architecture, same network is used for all the workers, however the parameters are different\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        \"\"\"\n",
        "        self.conv1.weight.data.fill_(0.01)\n",
        "        self.conv1.bias.data.fill_(0.01)\n",
        "        \n",
        "        self.conv2.weight.data.fill_(0.01)\n",
        "        self.conv2.bias.data.fill_(0.01)\n",
        "        \n",
        "        self.fc1.weight.data.fill_(0.01)\n",
        "        self.fc1.bias.data.fill_(0.01)\n",
        "        \n",
        "        self.fc2.weight.data.fill_(0.01)\n",
        "        self.fc2.bias.data.fill_(0.01)\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCETxiEC3xgH"
      },
      "outputs": [],
      "source": [
        "# Function for creating the workers, assigning models \n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) \n",
        "models = []\n",
        "workers = []\n",
        "\n",
        "def connect_to_workers(hook, n_workers):\n",
        "    workers = []\n",
        "    for i in range(n_workers):\n",
        "        worker = sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
        "        models.append(model.copy().send(worker))\n",
        "        workers.append(worker)\n",
        "    return workers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_KQzzC_32Dt"
      },
      "outputs": [],
      "source": [
        "# the forward and backward propagation for FedCollabNN Framework\n",
        "class FedCollabNN:\n",
        "    def __init__(self, models, optimizers):\n",
        "        self.models = models\n",
        "        self.optimizers = optimizers\n",
        "        self.criterion = nn.NLLLoss()\n",
        "        \n",
        "    def forward(self, data, target):\n",
        "        a = []\n",
        "        modelLoss = []\n",
        "        modelLosses = []\n",
        "        avg_loss = torch.tensor(0., requires_grad=False)\n",
        "        \n",
        "        for i in range (len(models)):\n",
        "            a.append(models[i](data[i]))\n",
        "            modelLoss = (self.criterion(a[i],target[i]))\n",
        "            modelLosses.append(modelLoss)\n",
        "            avg_loss = (avg_loss + modelLoss.copy().get()) #modelLoss.get()\n",
        "            \n",
        "        avg_loss = avg_loss/len(models)\n",
        "        self.a = a\n",
        "        self.modelLosses = modelLosses\n",
        "        self.avg_loss = avg_loss\n",
        "        \n",
        "        return avg_loss\n",
        "        \n",
        "    def backward(self):\n",
        "        a = self.a\n",
        "        avg_loss = self.avg_loss\n",
        "        modelLosses = self.modelLosses\n",
        "        optimizers = self.optimizers\n",
        "        \n",
        "        for i in range(len(models)):\n",
        "            grad_attributor = avg_loss.copy().send(models[i].location)\n",
        "            modelLosses[i].backward(grad_attributor/modelLosses[i])\n",
        "            \n",
        "            \n",
        "    def zero_grads(self):\n",
        "        for opt in optimizers:\n",
        "            opt.zero_grad()\n",
        "            \n",
        "    def step(self):\n",
        "        for opt in optimizers:\n",
        "            opt.step()\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcVlIKZj35nM"
      },
      "outputs": [],
      "source": [
        "# Train function for FedCollabNN framework\n",
        "def train(x, target, fedCollabNN):\n",
        "    \n",
        "    #1) Zero grads\n",
        "    fedCollabNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    avg_loss = fedCollabNN.forward(x, target)\n",
        "    \n",
        "    #4) Backprop the loss on the end layer\n",
        "    fedCollabNN.backward()\n",
        "    \n",
        "    #6) Change the weights\n",
        "    fedCollabNN.step()\n",
        "    \n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkRzME-23-hb"
      },
      "outputs": [],
      "source": [
        "# test function for all models\n",
        "def test(args, model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. *correct/ len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIpBSYx74C4e"
      },
      "outputs": [],
      "source": [
        "# test function for aggregated result, with and without global differential privacyy\n",
        "def testAggregate(args, models, test_loader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    correctdp = 0\n",
        "    aggregatePred = []\n",
        "    aggregatePreddp = []\n",
        "    pred=[]\n",
        "    teacher_preds=np.empty((len(models),10000), dtype=float)\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            for i in range(len(models)):\n",
        "                tempModel = models[i].get()\n",
        "                output = tempModel(data)\n",
        "                pred.append(output.argmax(1, keepdim=True)) # get the index of the max log-probability \n",
        "                teacher_preds[i,:]=np.reshape(np.asarray(pred[0]),(10000))\n",
        "                \n",
        "            for j in range(10000):\n",
        "                tempList = []\n",
        "                for i in range(len(models)):\n",
        "                    tempList.append((pred[i][j]).data.item())\n",
        "                \n",
        "                beta = 1/args.epsilon\n",
        "                label_counts = np.bincount(tempList, minlength=10)\n",
        "                label_counts_dp = label_counts\n",
        "                aggregatePred.append(np.argmax(label_counts)) \n",
        "                for k in range(10):\n",
        "                    label_counts_dp[k] += np.random.laplace(0,beta,1)    \n",
        "                aggregatePreddp.append(np.argmax(label_counts_dp))\n",
        "                \n",
        "            for i in range(10000):\n",
        "                if aggregatePred[i] == target[i]:\n",
        "                    correct += 1\n",
        "\n",
        "            for i in range(10000):\n",
        "                if aggregatePreddp[i] == target[i]:\n",
        "                    correctdp += 1\n",
        "    \n",
        "    \n",
        "    \n",
        "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    print('\\nTest set: Accuracy with Noise: {}/{} ({:.0f}%)\\n'.format(correctdp, len(test_loader.dataset),\n",
        "        100. * correctdp / len(test_loader.dataset)))\n",
        "    \n",
        "    return (100. * correct / len(test_loader.dataset)), (100. * correctdp / len(test_loader.dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Euv7QP9R4I9H"
      },
      "outputs": [],
      "source": [
        "totalWorkers =[10, 12, 14, 16, 18, 20]\n",
        "aggregateAccuracyDiffWorkers = np.empty((len(totalWorkers), args.epochs), dtype=float)\n",
        "index = 0\n",
        "aggregateAccuracy = np.empty((args.epochs), dtype = float)\n",
        "for noWorkers in totalWorkers: # training and testing with different number of workers\n",
        "    \n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "    models = []\n",
        "    workers = connect_to_workers(hook, noWorkers)\n",
        "\n",
        "    # Create optimisers for each model and link to them\n",
        "    optimizers = [\n",
        "        optim.SGD(model.parameters(), lr=0.03,)\n",
        "        for model in models\n",
        "    ]\n",
        "    \n",
        "    fedCollabNN = FedCollabNN(models, optimizers)\n",
        "    \n",
        "    # Distributing and sending the data and their corresponding labels to workers\n",
        "    train_distributed_dataset = []\n",
        "\n",
        "    n_max_batch = 500\n",
        "    n_train_items = n_max_batch * args.batch_size\n",
        "\n",
        "    for batch_idx, (data,target) in enumerate(train_loader):\n",
        "        if batch_idx < n_max_batch:\n",
        "            data = data.send(workers[batch_idx % len(workers)])\n",
        "            target = target.send(workers[batch_idx % len(workers)])\n",
        "            train_distributed_dataset.append((data, target))\n",
        "    \n",
        "    \n",
        "    workerTestAccuracies = np.empty((args.epochs,len(workers)), dtype=float)\n",
        "    \n",
        "    \n",
        "    for i in range(args.epochs): \n",
        "        imageTensor = []\n",
        "        labelTensor = []\n",
        "        running_loss = 0\n",
        "        idx=0\n",
        "        \n",
        "\n",
        "        for images, labels in train_distributed_dataset:\n",
        "            if (idx == len(workers)):# in every step a single batch for each workers is used to train models\n",
        "                loss = train(imageTensor, labelTensor, fedCollabNN)\n",
        "                running_loss += loss\n",
        "                imageTensor = []\n",
        "                labelTensor = []\n",
        "                idx = 0\n",
        "\n",
        "            imageTensor.append(images)\n",
        "            labelTensor.append(labels)\n",
        "            idx += 1\n",
        "\n",
        "        else:\n",
        "            print(\"Epoch {} - Training loss: {}\".format(i, running_loss*10/len(train_distributed_dataset)))\n",
        "        \n",
        "        # testing each model accuracyy after each epoch\n",
        "        finalModel = []\n",
        "        testAccuracies = []\n",
        "        for j in range(len(models)):\n",
        "            finalModel= models[j].get()\n",
        "            workerTestAccuracies[i,j] = test(args, finalModel, test_loader)\n",
        "            print(workerTestAccuracies[i,j])\n",
        "            finalModel.send(workers[j])\n",
        "        \n",
        "        # testing aggregated accuracyy after each epoch\n",
        "        temp, aggregateAccuracy[i] = testAggregate(args, models, test_loader)\n",
        "        for k in range(len(models)):\n",
        "            models[k].send(workers[k])\n",
        "    \n",
        "    aggregateAccuracyDiffWorkers[index, :] =aggregateAccuracy\n",
        "    index += 1\n",
        "    ### Plot commands for model accuracies with different number of workers\n",
        "    plt.figure()\n",
        "    for worker in range(noWorkers):\n",
        "        plt.plot(workerTestAccuracies[:,worker], label='worker '+ str(worker))\n",
        "    plt.xlabel(\"Epoch Number)\")\n",
        "    plt.ylabel(\"Percentage Accuracy\")\n",
        "    #plt.legend(framealpha=1, frameon=True)\n",
        "    plt.xticks(np.arange(1, 6, step=1))\n",
        "    plt.savefig(str(noWorkers) + '_' + str(worker) + 'workerTestAccuracies.eps', format='eps')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "### Plot commands for aggregated accuracies with different number of workers\n",
        "plt.figure()\n",
        "index=0\n",
        "for noWorkers in range(len(totalWorkers)):\n",
        "    plt.plot(aggregateAccuracyDiffWorkers[noWorkers, :], label= str(totalWorkers[index]) + 'workers')\n",
        "    index +=1\n",
        "    \n",
        "plt.xlabel(\"Epoch Number)\")\n",
        "plt.ylabel(\"Percentage Accuracy\")   \n",
        "plt.xticks(np.arange(1, 6, step=1))\n",
        "plt.legend(framealpha=1, frameon=True)\n",
        "plt.savefig('workerTestAggregateAccuracies.eps', format='eps')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfF8uPWh4qgMpivqM0QjHz",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}